{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "In this exercise we will implement some important parts of `Two-stage TrAdaBoost`.\n",
    "\n",
    "For more information check David Pardoe et al. 2010, *Boosting for Regression Transfer*, ICML 2010.\n",
    "\n",
    "We will implement three classes: `Stage2_TrAdaBoostR2` contains only Stage 2 of our Algorithm. `TwoStageTrAdaBoostR2` contains both Stages, using the class `Stage2_TrAdaBoostR2`.\n",
    "\n",
    "They both need boosting, a function provided by the `TrAdaBoostR2Base` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrAdaBoostR2Base:\n",
    "    def __init__(self,\n",
    "                 base_estimator = DecisionTreeRegressor(max_depth=4),\n",
    "                 sample_size = None,\n",
    "                 n_estimators = 50,\n",
    "                 learning_rate = 1.,\n",
    "                 loss = 'linear',\n",
    "                 random_state = np.random.mtrand._rand):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.sample_size = sample_size\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def boosting(self, X, y, sample_weight):\n",
    "        # first, we make a copy of our estimator\n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "                \n",
    "        # we use weighted sampling of the training set with replacement\n",
    "        # that means we want to have an equally sized training set\n",
    "        # but with some of them replaced by a copy\n",
    "        # not uniformly distributed but accounting the sample weights\n",
    "        # the higher its weight, the more often it is chosen as training sample\n",
    "        \n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "\n",
    "        # now we fit the chosen training sample set and get a prediction of X\n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "        \n",
    "        # calculate the absolute error vector of our prediction\n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "        \n",
    "        # normalize the error vector\n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "        \n",
    "        # calculate the error according to loss function\n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "        return estimator, error_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage2_TrAdaBoostR2(TrAdaBoostR2Base):\n",
    "    def __init__(self,\n",
    "                 base_estimator = DecisionTreeRegressor(max_depth=4),\n",
    "                 sample_size = None,\n",
    "                 n_estimators = 50,\n",
    "                 learning_rate = 1.,\n",
    "                 loss = 'linear',\n",
    "                 random_state = np.random.mtrand._rand):\n",
    "        super().__init__(base_estimator, sample_size, n_estimators, learning_rate, loss, random_state)\n",
    "\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        # Check parameters\n",
    "        if self.learning_rate <= 0:\n",
    "            raise ValueError(\"learning_rate must be greater than zero\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            # Initialize weights to 1 / n_samples\n",
    "            sample_weight = np.empty(X.shape[0], dtype=np.float64)\n",
    "            sample_weight[:] = 1. / X.shape[0]\n",
    "        else:\n",
    "            # Normalize existing weights\n",
    "            sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n",
    "\n",
    "            # Check that the sample weights sum is positive\n",
    "            if sample_weight.sum() <= 0:\n",
    "                raise ValueError(\n",
    "                      \"Attempting to fit with a non-positive \"\n",
    "                      \"weighted number of samples.\")\n",
    "\n",
    "        if self.sample_size is None:\n",
    "            raise ValueError(\"Additional input required: sample size of source and target is missing\")\n",
    "        elif np.array(self.sample_size).sum() != X.shape[0]:\n",
    "            raise ValueError(\"Input error: the specified sample size does not equal to the input size\")\n",
    "\n",
    "        # Clear any previous fit results\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)\n",
    "\n",
    "        for iboost in range(self.n_estimators): # this for loop has to be sequencial\n",
    "            # Boosting step\n",
    "            sample_weight, estimator_weight, estimator_error = self._stage2_adaboostR2(\n",
    "                    iboost,\n",
    "                    X, y,\n",
    "                    sample_weight)\n",
    "            # Early termination\n",
    "            if sample_weight is None:\n",
    "                break\n",
    "\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "            # Stop if error is zero\n",
    "            if estimator_error == 0:\n",
    "                break\n",
    "\n",
    "            sample_weight_sum = np.sum(sample_weight)\n",
    "\n",
    "            # Stop if the sum of sample weights has become non-positive\n",
    "            if sample_weight_sum <= 0:\n",
    "                break\n",
    "\n",
    "            if iboost < self.n_estimators - 1:\n",
    "                # Normalize\n",
    "                sample_weight /= sample_weight_sum\n",
    "        return self\n",
    "                \n",
    "    def _stage2_adaboostR2(self, iboost, X, y, sample_weight):\n",
    "        estimator, error_vect = self.boosting(X, y, sample_weight)\n",
    "\n",
    "        # add the fitted estimator to our estimators list\n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "\n",
    "        # Calculate the weighed accumulated estimator error\n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "\n",
    "        # Stop if fit is perfect\n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "        \n",
    "        # Remove estimator if estimator error is >= 0.5\n",
    "        # and if it's not the only one\n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "        \n",
    "        # calculate the new beta_t\n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "\n",
    "        # avoid overflow of np.log(1. / beta)\n",
    "        if beta < 1e-308:\n",
    "            beta = 1e-308\n",
    "        \n",
    "        # calculate the new weight for hypothesis h_t\n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "\n",
    "        # update the weight vectors according to AdaBoost.R2 except the weights of the source data\n",
    "        # use the sum of all sample weights as normalizing constant\n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "\n",
    "        return sample_weight, estimator_weight, estimator_error\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Evaluate predictions of all estimators\n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "        \n",
    "        # Sort the predictions and get indices\n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "        \n",
    "        # Find index of median prediction for each sample\n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "        \n",
    "        # Get the median estimators\n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "        \n",
    "        # Get the median predictions\n",
    "        ####################\n",
    "        # Your Code Here   #\n",
    "        ####################\n",
    "        \n",
    "        # Return median predictions\n",
    "        return median_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to compare it to `Two-stage TrAdaBoostR2`, which is already implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStageTrAdaBoostR2(TrAdaBoostR2Base):\n",
    "    def __init__(self,\n",
    "                 base_estimator = DecisionTreeRegressor(max_depth=4),\n",
    "                 sample_size = None,\n",
    "                 n_estimators = 50,\n",
    "                 steps = 10,\n",
    "                 fold = 5,\n",
    "                 learning_rate = 1.,\n",
    "                 loss = 'linear',\n",
    "                 random_state = np.random.mtrand._rand):\n",
    "        self.steps = steps\n",
    "        self.fold = fold\n",
    "        super().__init__(base_estimator, sample_size, n_estimators, learning_rate, loss, random_state)\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        # Check parameters\n",
    "        if self.learning_rate <= 0:\n",
    "            raise ValueError(\"learning_rate must be greater than zero\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            # Initialize weights to 1 / n_samples\n",
    "            sample_weight = np.empty(X.shape[0], dtype=np.float64)\n",
    "            sample_weight[:] = 1. / X.shape[0]\n",
    "        else:\n",
    "            # Normalize existing weights\n",
    "            sample_weight /= sample_weight.sum(dtype=np.float64)\n",
    "\n",
    "            # Check that the sample weights sum is positive\n",
    "            if sample_weight.sum() <= 0:\n",
    "                raise ValueError(\n",
    "                      \"Attempting to fit with a non-positive \"\n",
    "                      \"weighted number of samples.\")\n",
    "\n",
    "        if self.sample_size is None:\n",
    "            raise ValueError(\"Additional input required: sample size of source and target is missing\")\n",
    "        elif np.array(self.sample_size).sum() != X.shape[0]:\n",
    "            raise ValueError(\"Input error: the specified sample size does not equal to the input size\")\n",
    "\n",
    "\n",
    "        X_source = X[:-self.sample_size[-1]]\n",
    "        y_source = y[:-self.sample_size[-1]]\n",
    "        X_target = X[-self.sample_size[-1]:]\n",
    "        y_target = y[-self.sample_size[-1]:]\n",
    "\n",
    "        self.models_ = []\n",
    "        self.errors_ = []\n",
    "        for istep in range(self.steps):\n",
    "            model = Stage2_TrAdaBoostR2(self.base_estimator,\n",
    "                                        sample_size = self.sample_size,\n",
    "                                        n_estimators = self.n_estimators,\n",
    "                                        learning_rate = self.learning_rate, loss = self.loss,\n",
    "                                        random_state = self.random_state)\n",
    "            model.fit(X, y, sample_weight = sample_weight)\n",
    "            self.models_.append(model)\n",
    "            # cv training\n",
    "            kf = KFold(n_splits = self.fold)\n",
    "            error = []\n",
    "            target_weight = sample_weight[-self.sample_size[-1]:]\n",
    "            source_weight = sample_weight[:-self.sample_size[-1]]\n",
    "            for train, test in kf.split(X_target):\n",
    "                sample_size = [self.sample_size[0], len(train)]\n",
    "                model = Stage2_TrAdaBoostR2(self.base_estimator,\n",
    "                                        sample_size = sample_size,\n",
    "                                        n_estimators = self.n_estimators,\n",
    "                                        learning_rate = self.learning_rate, loss = self.loss,\n",
    "                                        random_state = self.random_state)\n",
    "                X_train = np.concatenate((X_source, X_target[train]))\n",
    "                y_train = np.concatenate((y_source, y_target[train]))\n",
    "                X_test = X_target[test]\n",
    "                y_test = y_target[test]\n",
    "                # make sure the sum weight of the target data do not change with CV's split sampling\n",
    "                target_weight_train = target_weight[train]*np.sum(target_weight)/np.sum(target_weight[train])\n",
    "                model.fit(X_train, y_train, sample_weight = np.concatenate((source_weight, target_weight_train)))\n",
    "                y_predict = model.predict(X_test)\n",
    "                error.append(mean_squared_error(y_predict, y_test))\n",
    "\n",
    "            self.errors_.append(np.array(error).mean())\n",
    "\n",
    "            sample_weight = self._twostage_adaboostR2(istep, X, y, sample_weight)\n",
    "\n",
    "            if sample_weight is None:\n",
    "                break\n",
    "            if np.array(error).mean() == 0:\n",
    "                break\n",
    "\n",
    "            sample_weight_sum = np.sum(sample_weight)\n",
    "\n",
    "            # Stop if the sum of sample weights has become non-positive\n",
    "            if sample_weight_sum <= 0:\n",
    "                break\n",
    "\n",
    "            if istep < self.steps - 1:\n",
    "                # Normalize\n",
    "                sample_weight /= sample_weight_sum\n",
    "        return self\n",
    "\n",
    "\n",
    "    def _twostage_adaboostR2(self, istep, X, y, sample_weight):\n",
    "        estimator, error_vect = self.boosting(X, y, sample_weight)\n",
    "\n",
    "        # Update the weight vector\n",
    "        beta = self._beta_binary_search(istep, sample_weight, error_vect, stp = 1e-30)\n",
    "\n",
    "        if not istep == self.steps - 1:\n",
    "            sample_weight[:-self.sample_size[-1]] *= np.power(\n",
    "                    beta,\n",
    "                    (error_vect[:-self.sample_size[-1]]) * self.learning_rate)\n",
    "        return sample_weight\n",
    "\n",
    "\n",
    "    def _beta_binary_search(self, istep, sample_weight, error_vect, stp):\n",
    "        # calculate the specified sum of weight for the target data\n",
    "        n_target = self.sample_size[-1]\n",
    "        n_source = np.array(self.sample_size).sum() - n_target\n",
    "        theoretical_sum = n_target/(n_source+n_target) + istep/(self.steps-1)*(1-n_target/(n_source+n_target))\n",
    "        # for the last iteration step, beta is 0.\n",
    "        if istep == self.steps - 1:\n",
    "            beta = 0.\n",
    "            return beta\n",
    "        # binary search for beta\n",
    "        L = 0.\n",
    "        R = 1.\n",
    "        beta = (L+R)/2\n",
    "        sample_weight_ = copy.deepcopy(sample_weight)\n",
    "        sample_weight_[:-n_target] *= np.power(\n",
    "                    beta,\n",
    "                    (error_vect[:-n_target]) * self.learning_rate)\n",
    "        sample_weight_ /= np.sum(sample_weight_, dtype=np.float64)\n",
    "        updated_weight_sum = np.sum(sample_weight_[-n_target:], dtype=np.float64)\n",
    "\n",
    "        while np.abs(updated_weight_sum - theoretical_sum) > 0.01:\n",
    "            if updated_weight_sum < theoretical_sum:\n",
    "                R = beta - stp\n",
    "                if R > L:\n",
    "                    beta = (L+R)/2\n",
    "                    sample_weight_ = copy.deepcopy(sample_weight)\n",
    "                    sample_weight_[:-n_target] *= np.power(\n",
    "                                beta,\n",
    "                                (error_vect[:-n_target]) * self.learning_rate)\n",
    "                    sample_weight_ /= np.sum(sample_weight_, dtype=np.float64)\n",
    "                    updated_weight_sum = np.sum(sample_weight_[-n_target:], dtype=np.float64)\n",
    "                else:\n",
    "                    # binary search goal not met, try to reduce the search inverval in the next iteration\n",
    "                    break\n",
    "\n",
    "            elif updated_weight_sum > theoretical_sum:\n",
    "                L = beta + stp\n",
    "                if L < R:\n",
    "                    beta = (L+R)/2\n",
    "                    sample_weight_ = copy.deepcopy(sample_weight)\n",
    "                    sample_weight_[:-n_target] *= np.power(\n",
    "                                beta,\n",
    "                                (error_vect[:-n_target]) * self.learning_rate)\n",
    "                    sample_weight_ /= np.sum(sample_weight_, dtype=np.float64)\n",
    "                    updated_weight_sum = np.sum(sample_weight_[-n_target:], dtype=np.float64)\n",
    "                else:\n",
    "                    # binary search goal not met, try to reduce the search inverval in the next iteration\n",
    "                    break\n",
    "        return beta\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        # select the model with the least CV error\n",
    "        fmodel = self.models_[np.array(self.errors_).argmin()]\n",
    "        predictions = fmodel.predict(X)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the differences? \n",
    "\n",
    "Now we can test our implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(x, d, random_state):\n",
    "    \"\"\"\n",
    "    x is the input variable\n",
    "    d controls the simularity of different tasks\n",
    "    \"\"\"\n",
    "    c = []\n",
    "    factor_weights = [0.1, 0.1, 0.1 , 0.1, 0.05, 0.05]\n",
    "    \n",
    "    for weight in factor_weights:\n",
    "        c.append(np.random.normal(1, weight*d))\n",
    "\n",
    "    y = c[0]*np.sin(c[2]*x + c[4]).ravel() + c[1]*np.sin(c[3]*6 * x + c[5]).ravel() + random_state.normal(0, 0.1, x.shape[0])\n",
    "    return y\n",
    "\n",
    "d = 0.5\n",
    "signals_count = 5\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "n_source = []\n",
    "x_source = []\n",
    "y_source = []\n",
    "\n",
    "for i in range(signals_count):\n",
    "    n_source.append(100)\n",
    "    x_source.append(np.linspace(0, 6, n_source[i])[:, np.newaxis])\n",
    "    y_source.append(response(x_source[i], d, rng))\n",
    "\n",
    "n_target_train = 15\n",
    "\n",
    "x_target_train = np.linspace(0, 6, n_target_train)[:, np.newaxis]\n",
    "y_target_train = response(x_target_train, d, rng)\n",
    "\n",
    "n_target_test = 600\n",
    "x_target_test = np.linspace(0, 6, n_target_test)[:, np.newaxis]\n",
    "y_target_test = response(x_target_test, d, rng)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for i in range(signals_count):\n",
    "    plt.plot(x_source[i], y_source[i], label=\"source\"+str(i), linewidth=1)\n",
    "\n",
    "plt.plot(x_target_test, y_target_test, c=\"b\", label=\"target_test\", linewidth=0.5)\n",
    "plt.scatter(x_target_train, y_target_train, c=\"k\", label=\"target_train\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Multiple datasets\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "X = copy.deepcopy(x_source[0])\n",
    "for i in range(1,signals_count):\n",
    "    X = np.concatenate((X,x_source[i]))\n",
    "\n",
    "X = np.concatenate((X, x_target_train))\n",
    "\n",
    "y = copy.deepcopy(y_source[0])\n",
    "for i in range(1,signals_count):\n",
    "    y = np.concatenate((y,y_source[i]))\n",
    "\n",
    "y = np.concatenate((y, y_target_train))\n",
    "\n",
    "sample_size = [np.sum(n_source), n_target_train]\n",
    "\n",
    "n_estimators = 100\n",
    "steps = 10\n",
    "fold = signals_count\n",
    "random_state = np.random.RandomState(1)\n",
    "\n",
    "regr_1 = TwoStageTrAdaBoostR2(DecisionTreeRegressor(max_depth=6),\n",
    "                      n_estimators = n_estimators, sample_size = sample_size, \n",
    "                      steps = steps, fold = fold, \n",
    "                      random_state = random_state)\n",
    "regr_1.fit(X, y)\n",
    "y_pred1 = regr_1.predict(x_target_test)\n",
    "\n",
    "# test AdaBoostR2 without transfer learning\n",
    "regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=6),\n",
    "                          n_estimators = n_estimators)\n",
    "\n",
    "regr_2.fit(x_target_train, y_target_train)\n",
    "y_pred2 = regr_2.predict(x_target_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x_target_train, y_target_train, c=\"k\", label=\"target_train\")\n",
    "plt.plot(x_target_test, y_target_test, c=\"b\", label=\"target_test\", linewidth=0.5)\n",
    "plt.plot(x_target_test, y_pred1, c=\"r\", label=\"TwoStageTrAdaBoostR2\", linewidth=2)\n",
    "plt.plot(x_target_test, y_pred2, c=\"y\", label=\"AdaBoostR2\", linewidth=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Two-stage Transfer Learning vs. Boosted Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mse_twostageboost = mean_squared_error(y_target_test, y_pred1)   \n",
    "mse_adaboost = mean_squared_error(y_target_test, y_pred2)\n",
    "\n",
    "print(\"MSE of regular AdaboostR2:\", mse_adaboost)\n",
    "print(\"MSE of TwoStageTrAdaboostR2:\", mse_twostageboost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out different values for the variable $d$, i.e. different levels of similarity of our source data. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
