{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "MathJax.Hub.Config({\n",
       "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
       "});"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "MathJax.Hub.Config({\n",
    "    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General remarks\n",
    "This notebook introduces the mathematical foundations for various linear regression approaches.\n",
    "It is up to **you** to:\n",
    "1. extend the notebook with python code to train a linear regression model and\n",
    "2. to plot the linear regressions model for one-dimensional or two-dimensional input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression is a linear approach that allows us to analyze the relationship between a dependent variable and one or more explanatory variables. \n",
    "\n",
    "## Two simple cases of Linear Regression\n",
    "\n",
    "The simplest model for linear regression is a linear combination of the elements of $\\mathbf{x}$, with ${\\phi}_{j}(\\mathbf{x}) = \\mathit{x}_{j}$ for j>0 and ${\\phi}_{0}(\\mathbf{x}) = 1$,\n",
    "\n",
    "$$ y(\\mathbf{x},\\mathbf{w}) = \\mathit{w}_0 + \\mathit{w}_1 \\cdot \\mathit{x}_\\mathit{1} + \\mathit{w}_2 \\cdot \\mathit{x}_\\mathit{2} + ... + \\mathit{w}_\\mathit{D} \\cdot \\mathit{x}_\\mathit{D} $$\n",
    "\n",
    "where $\\mathbf{x} = (\\mathit{x}_{1},...,\\mathit{x}_{\\mathit{D}})^\\mathrm{T}$.\n",
    "\n",
    "A model often presented in literature is the linear regression of one-dimensional observations using a regression line.  \n",
    "Here, the dimensionality $\\mathit{D}$ equals one, and, therefore, $\\mathbf{x} = (1, \\mathit{x}_{1})^\\mathrm{T}$, resulting in the equation of a line:\n",
    "\n",
    "$$ y(\\mathbf{x},\\mathbf{w}) = \\mathit{w}_0 + \\mathit{w}_1 \\cdot \\mathit{x}_\\mathit{1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Method\n",
    "\n",
    "For fitting the parameters $\\mathbf{w} = (\\mathit{w}_{0},...,\\mathit{w}_{M-1})^\\mathrm{T}$ of a function $y(\\mathbf{x},\\mathbf{w})$ to a data set of N observations {$\\mathbf{x}_n$} and their corresponding target values {$t_n$}, one can apply the least squares method. It aims to find the values for $\\mathbf{w}$ that minimize the sum of squared distances between the measured target values {$t_n$} and the respective predicted target values $\\{ \\hat {t}_n\\}  = \\{y(\\mathbf{x}_n,\\mathbf{w})\\}$.\n",
    "\n",
    "\\begin{equation}\n",
    "S(\\mathbf{w}) = \\sum_{n=1}^{N}\\left(\\hat{t}_n - t_n\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "## Least Squares Solution for a Regression Line\n",
    "\n",
    "With the regression line we have to determine the values for the two parameters $\\mathit{w}_0$ and $\\mathit{w}_1$ that minimize the sum of the squared distances. The input vector $\\mathbf{x}$ has only one element, $x_\\mathit{1}$. To avoid unnecessary clutter in the equations, we rename $x_\\mathit{1}$ to $x$ in this section.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\min\\ S(\\mathbf{w}) & = \\min \\sum_{n=1}^{N}({\\mathbf{\\hat{t}}_n}-{t_n})^2 \\\\\n",
    " & = \\min \\sum_{n=1}^{N}({\\mathit{w}_0 + \\mathit{w}_1 \\cdot \\mathit{x}_\\mathit{n}}-{t_n})^2\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "For this we set the partial differential equations $\\frac{\\partial S}{\\partial \\mathit{w}_0}$ and $\\frac{\\partial S}{\\partial \\mathit{w}_1}$ equal to zero:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial S}{\\partial \\mathit{w}_0} &= -2 \\cdot \\sum_{n=1}^{N}(t_n - \\mathit{w}_0 - \\mathit{w}_1 \\cdot \\mathit{x}_\\mathit{n}) &= 0 \\\\\n",
    "\\frac{\\partial S}{\\partial \\mathit{w}_1} &= -2 \\cdot \\sum_{n=1}^{N} \\mathit{x}_\\mathit{n} (t_n - \\mathit{w}_0 - \\mathit{w}_1 \\cdot \\mathit{x}_\\mathit{n}) &= 0\n",
    "\\end{align}\n",
    "\n",
    "We now find the values of $\\mathit{w}_0$ and $\\mathit{w}_1$ for wich the partial differential equations become zero, which we name $\\mathit{w}_{0}^{opt}$ and $\\mathit{w}_{1}^{opt}$:\n",
    "\n",
    "\\begin{align}\n",
    " & \\sum_{n=1}^{N}(t_n - \\mathit{w}_{0}^{opt} - \\mathit{w}_{1}^{opt}\\mathit{x}_\\mathit{n}) &= 0 \\\\\n",
    " & \\sum_{n=1}^{N}(t_n\\mathit{x}_\\mathit{n} - \\mathit{w}_{0}^{opt}\\mathit{x}_\\mathit{n} - \\mathit{w}_{1}^{opt}\\mathit{x}_\\mathit{n}^2) &= 0\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "From (5) we get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{n=1}^{N} t_n - N\\cdot\\mathit{w}_{0}^{opt} - \\mathit{w}_{1}^{opt} \\sum_{n=1}^{N} \\mathit{x}_\\mathit{n} = 0\n",
    "\\end{equation}\n",
    "\n",
    "With $\\bar{t}=\\frac{1}{N}\\sum_{n=1}^{N}t_n$, $\\ \\mathit{\\bar{x}}_\\mathit{1}=\\frac{1}{N}\\sum_{n=1}^{N}\\mathit{x}_\\mathit{n}\\ $, and dividing (7) by $N$ we get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{t} = \\mathit{w}_{0}^{opt} + \\mathit{w}_{1}^{opt}\\mathit{\\bar{x}}_\\mathit{1}\n",
    "\\end{equation}\n",
    "\n",
    "It is an interesting intermediate result, that the center point of the data set, $(\\mathit{\\bar{x}}_\\mathit{1},\\bar{t})$, lies on the regression line.\n",
    "\n",
    "From (6) we get:\n",
    "\n",
    "\\begin{align}\n",
    "& \\sum_{n=1}^{N}t_n\\mathit{x}_\\mathit{n} - \\sum_{n=1}^{N}\\mathit{w}_0\\mathit{x}_\\mathit{n} - \\sum_{n=1}^{N}\\mathit{w}_{1}^{opt}\\mathit{x}_\\mathit{n}^2 &= 0 \\\\\n",
    "& \\sum_{n=1}^{N}t_n\\mathit{x}_\\mathit{n} - N\\mathit{w}_{0}^{opt}\\mathit{\\bar{x}}_\\mathit{1} - \\mathit{w}_{1}^{opt}\\sum_{n=1}^{N}\\mathit{x}_\\mathit{n}^2 &= 0 \\\\\n",
    "\\end{align}\n",
    "\n",
    "Together with (8), we get\n",
    "\n",
    "\\begin{align}\n",
    "&\\sum_{n=1}^{N}t_n\\mathit{x}_\\mathit{n} - N\\mathit{\\bar{x}}_\\mathit{1}(\\bar{t}-\\mathit{w}_{1}^{opt}\\mathit{\\bar{x}}_\\mathit{1}) &=& \\mathit{w}_{1}^{opt}\\sum_{n=1}^{N}\\mathit{x}_\\mathit{n}^2 \\\\\n",
    "&\\sum_{n=1}^{N}t_n\\mathit{x}_\\mathit{n} - N\\mathit{\\bar{x}}_\\mathit{1}\\bar{t} &=& \\mathit{w}_{1}^{opt} \\left( \\sum_{n=1}^{N}\\mathit{x}_\\mathit{n}^2 - N\\mathit{\\bar{x}}_\\mathit{1}^2\\right)\n",
    "\\end{align}\n",
    "\n",
    ", which can be resolved to $\\mathit{w}_{1}^{opt}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathit{w}_{1}^{opt} = \\frac{\\sum_{n=1}^{N}t_n\\mathit{x}_\\mathit{n} - N\\mathit{\\bar{x}}_\\mathit{1}\\bar{t}}{\\sum_{n=1}^{N}\\mathit{x}_\\mathit{n}^2 - N\\mathit{\\bar{x}}_\\mathit{1}^2}\n",
    "\\end{equation}\n",
    "\n",
    "So, the solution for a line regression with the least squares method is:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathit{w}_{0}^{opt} &= \\bar{t} - \\mathit{w}_{1}^{opt}\\mathit{\\bar{x}}_\\mathit{1} \\\\\n",
    "\\mathit{w}_{1}^{opt} &= \\frac{\\sum_{n=1}^{N}t_n\\mathit{x}_\\mathit{n} - N\\mathit{\\bar{x}}_\\mathit{1}\\bar{t}}{\\sum_{n=1}^{N}\\mathit{x}_\\mathit{n}^2 - N\\mathit{\\bar{x}}_\\mathit{1}^2}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "For $N \\geq 2$ we can write the *sample variance* of $\\mathit{x}_\\mathit{1}$ as\n",
    "\n",
    "\\begin{equation}\n",
    "s_{x_1}^2 = \\frac{1}{N-1}( \\sum_{n=1}^{N}\\mathit{x}_\\mathit{n}^2 - N\\mathit{\\bar{x}}_\\mathit{1}^2 )\n",
    "\\end{equation}\n",
    "\n",
    "and the *sample covariance* of $t$ and $x_1$ as\n",
    "\n",
    "\\begin{equation}\n",
    "s_{x_1,t} = \\mathrm{cov}(x_1,t) = \\frac{1}{N-1}\\left( \\sum_{n=1}^{N}t_n\\mathit{x}_\\mathit{n} - N\\mathit{\\bar{x}}_\\mathit{1}\\bar{t} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "With (14) to (17) we can write the solution in a shorter manner:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathit{w}_{0}^{opt} &= \\bar{t} - \\frac{s_{x_1,t}}{s_{x_1}^2} \\cdot \\mathit{\\bar{x}}_\\mathit{1} \\\\\n",
    "\\mathit{w}_{1}^{opt} &= \\frac{s_{x_1,t}}{s_{x_1}^2}\n",
    "\\end{align}\n",
    "\n",
    "Up to this point we only know that the result $(\\mathit{w}_{0}^{opt},\\mathit{w}_{1}^{opt})$ presents an optimum, not if it is a maximum, minimum, or saddle point. We have to check if it is actually a minimum by applying the second partial derivative test for functions of two variables. For this we need the second partial derivative $\\frac{\\partial{^2} S}{\\partial \\mathit{w}_{0}^2}$ and the determinant of the Hessian matrix of $S$, $\\det(H(\\mathit{w}_{0},\\mathit{w}_{1}))$. If the following two conditions are met, $(\\mathit{w}_{0}^{opt},\\mathit{w}_{1}^{opt})$ presents a minimum:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial{^2} S}{\\partial \\mathit{w}_{0}^2}\\ \\mathrm{is\\ positive\\ for}\\ (\\mathit{w}_{0}^{opt},\\mathit{w}_{1}^{opt}).\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  D(\\mathit{w}_{0},\\mathit{w}_{1})=\\det(H(\\mathit{w}_{0},\\mathit{w}_{1})) = \\frac{\\partial{^2} S}{\\partial \\mathit{w}_{0}^2} \\frac{\\partial{^2} S}{\\partial \\mathit{w}_{1}^2} - \\left(\\frac{\\partial{^2} S}{\\partial \\mathit{w}_{0} \\partial \\mathit{w}_{1}}\\right)^2 \\ \\mathrm{is\\ positive\\ for}\\ (\\mathit{w}_{0}^{opt},\\mathit{w}_{1}^{opt}).\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple regression model\n",
    "\n",
    "A city has several districts. For six districts (from A to F), the number of people working residents and the traffic density is known.\n",
    "\n",
    "We model a linear relation in the form of a regression line \n",
    "\n",
    "$\\mathrm{Traffic} = \\mathit{w}_1 * \\mathrm{Workers} + \\mathit{w}_0$ \n",
    "\n",
    "This allows predictions for districts with different numbers of workers for which no empiric data is present.\n",
    "\n",
    "| District | Workers | Traffic |\n",
    "|:--------:|:-------:|:-------:|\n",
    "|     A    |   4980  |  30700  |\n",
    "|     B    |   690   |   6800  |\n",
    "|     C    |   942   |  11500  |\n",
    "|     D    |   357   |   6200  |\n",
    "|     E    |  11622  |  76300  |\n",
    "|     F    |   1361  |   9100  |\n",
    "\n",
    "### Your tasks:\n",
    "1. learn the linear model with the method of least squares\n",
    "2. display the resulting trained parameters ($\\mathit{w}_0$ and  $\\mathit{w}_1$).\n",
    "3. plot the data points\n",
    "3. plot the linear model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# initialize Workers\n",
    "x = (4980,690,942,357,11622,1361)\n",
    "\n",
    "# initialize Traffic\n",
    "y = (30700,6800,11500,6200,76300,9100)\n",
    "\n",
    "\n",
    "# fit a regression model using x as input and y as output\n",
    "# YOUR CODE HERE\n",
    "# ... \n",
    "\n",
    "# print the model\n",
    "# YOUR CODE HERE\n",
    "# ... \n",
    "\n",
    "\n",
    "# plot the Traffic/Workers data points\n",
    "# YOUR CODE HERE\n",
    "# ... \n",
    "\n",
    "# plot the regression line\n",
    "# YOUR CODE HERE\n",
    "# ... \n",
    "\n",
    "# BONUS\n",
    "# plot distances in y from data points to the regression line\n",
    "# YOUR CODE HERE\n",
    "# ... \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregression\n",
    "\n",
    "The autoregressive (AR) model is used to describe certain time-varying processes. It specifies that the output variable depends linearly on its **own** previous values and a stochastic term.\n",
    "The following example shows an autoregression with a regression line for a process described by \n",
    "$y(t) = \\mathit{w}_1 * t + \\mathit{w}_0$ \n",
    "with added Gaussian noise. Note that here $t$ is not a target value, but the time at which an output $y$ occured.\n",
    "\n",
    "\n",
    "In the follwing example we generate a time-varing process:\n",
    "$y(t) = 0.7 * t + 2$\n",
    "\n",
    "### Your tasks:\n",
    "1. learn the linear model with the method of least squares\n",
    "2. display the resulting trained parameters ($\\mathit{w}_0$ and  $\\mathit{w}_1$).\n",
    "3. plot the data points\n",
    "3. plot the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# line parameters\n",
    "w0 = 2\n",
    "w1 = 0.7\n",
    "\n",
    "# standard deviation of Gaussian noise\n",
    "sigma = 0.5\n",
    "\n",
    "# initialize x as timeseries from 0 to 21\n",
    "t = np.arange(0,21,1)\n",
    "\n",
    "# create samples of y based on the line parameters\n",
    "y = np.zeros(t.size)\n",
    "for index in range(y.size) :\n",
    "    y[index] = w1 * t[index] + w0\n",
    "    \n",
    "# add Gaussian noise to y\n",
    "y = y + sigma * np.random.randn(y.size)\n",
    "\n",
    "# fit a regression model using t as input and y as output\n",
    "# YOUR CODE HERE\n",
    "# ... \n",
    "\n",
    "# print the model\n",
    "# YOUR CODE HERE\n",
    "# ... \n",
    "\n",
    "\n",
    "# plot the Traffic/Workers data points\n",
    "# YOUR CODE HERE\n",
    "# ... \n",
    "\n",
    "# plot the regression line\n",
    "# YOUR CODE HERE\n",
    "# ... \n",
    "\n",
    "# BONUS\n",
    "# plot distances in y from data points to the regression line\n",
    "# YOUR CODE HERE\n",
    "# ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Basis Function Models\n",
    "\n",
    "(see **Bishop**, \"Pattern Recognition and Machine Learning\", Springer 2006, Sec. 3.1)\n",
    "\n",
    "Given a training data set of N observations {$\\mathbf{x}_n$} and their corresponding target values {$t_n$}, where n=1..N, directly constructing an appropriate function $y(\\mathbf{x})$ to predict the corresponding values of $t$ for observations $\\mathbf{x}$ not part of the training data set can prove difficult in many cases.\n",
    "\n",
    "One way of approaching this problem is modelling $y(\\textbf{x})$ as a **linear** combination of functions of $\\mathbf{x}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y(\\mathbf{x},\\mathbf{w}) & = \\mathit{w}_0 + \\mathit{w}_1 \\cdot {\\phi}_{1}(\\mathbf{x}) + \\mathit{w}_2 \\cdot {\\phi}_{2}(\\mathbf{x}) + ... + \\mathit{w}_{M-1} \\cdot {\\phi}_{M-1}(\\mathbf{x}) \\\\\n",
    " & = \\mathit{w}_0 + \\sum_{j=1}^{M-1} \\mathit{w}_j \\phi_{j}(\\mathbf{x})\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{x} = (\\mathit{x}_{1},\\dotsc,\\mathit{x}_{\\mathit{D}})^\\mathrm{T}$.\n",
    "$\\mathit{D}$ is the dimensionality of the observations.\n",
    "The fixed functions ${\\phi}_{j}(\\mathbf{x})$ are known as *basis* functions.  \n",
    "The parameter $\\mathit{w}_0$ allows to model a fixed offset in the data.\n",
    "Including $\\mathit{w}_0$, the model has a number of **M** parameters.\n",
    "\n",
    "By defining a dummy basis function ${\\phi}_{0}(\\mathbf{x}) = 1$, we get\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y(\\mathbf{x},\\mathbf{w}) & = \\mathit{w}_0 \\cdot \\phi_{0}(\\mathbf{x}) + \\mathit{w}_1 \\cdot \\phi_{1}(\\mathbf{x}) + \\mathit{w}_2 \\cdot \\phi_{2}(\\mathbf{x}) + ... + \\mathit{w}_{M-1} \\cdot \\phi_{M-1}(\\mathbf{x}) \\\\\n",
    " & = \\sum_{j=0}^{M-1} \\mathit{w}_j \\phi_{j}(\\mathbf{x}) = \\mathbf{w}^\\mathrm{T} \\boldsymbol{\\phi}(\\mathbf{x})\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{w} = (\\mathit{w}_{0},...,\\mathit{w}_{M-1})^\\mathrm{T}$ and $\\boldsymbol{\\phi} = ({\\phi}_{0},...,{\\phi}_{M-1})^\\mathrm{T}$.\n",
    "\n",
    "---\n",
    "\n",
    "**Transposed Vectors and Matrices**\n",
    "\n",
    "In the text above a vector is sometimes written with $\\mathrm{T}$ as exponent, like $\\mathbf{w}^\\mathrm{T}$.\n",
    "$\\mathrm{T}$ is the **transpose** operator.\n",
    "Applied to a matrix $\\mathrm{T}$ switches the row and column indices, flipping the matrix over its diagonal:\n",
    "\n",
    "$$\\left[\\mathbf {A} ^{\\mathrm {T} }\\right]_{ij}=\\left[\\mathbf {A} \\right]_{ji}$$\n",
    "\n",
    "If $\\mathbf{A}$ is an $m$ x $n$ matrix, then $\\mathbf {A} ^{\\mathrm {T}}$ is an $n$ x $m$ matrix.\n",
    "An example:\n",
    "\n",
    "$$\n",
    "{\\begin{bmatrix} \n",
    "1 & 2 \\\\ \n",
    "3 & 4 \\\\ \n",
    "5 & 6 \\\\ \n",
    "\\end{bmatrix}}^{\\mathrm{ T}}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "1 & 3 & 5 \\\\ \n",
    "2 & 4 & 6 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Since a vector is a matrix with either a single row (row vector) or a single column (column vector), the transpose operation is also applicable to a vector.\n",
    "Applied to a column vector the result is a row vector and vice versa.\n",
    "An example:\n",
    "\n",
    "$$\n",
    "{\\begin{bmatrix} \n",
    "1 & 2 & 3 & 4\n",
    "\\end{bmatrix}}^{\\mathrm{ T}}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "4 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Basis function examples\n",
    "\n",
    "**Valid:**  \n",
    "$y(\\mathbf{x},\\mathbf{w}) = \\mathit{w}_0 + \\mathit{w}_1 sin(\\mathbf{x}) + \\mathit{w}_2 cos(\\mathbf{x})$  \n",
    "The basis function $sin(x)$ and $cos(x)$ are fixed.\n",
    "\n",
    "**Not valid:**  \n",
    "$y(\\mathbf{x},\\mathbf{w}) = \\mathit{w}_0 + \\mathit{w}_1 sin(\\mathit{w}_1\\ \\mathbf{x}) + \\mathit{w}_2 cos(\\mathbf{x})$  \n",
    "$sin(\\mathit{w}_1\\ \\mathbf{x})$ the parameter $\\mathit{w}_1$ doesn't go into $y(\\mathbf{x},\\mathbf{w})$ in a linear fashion.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Notation\n",
    "\n",
    "This section show an alternative matrix-based notation of a linear regression solved with the least-squares approach.\n",
    "\n",
    "\\begin{align}\n",
    "   \\boldsymbol{\\Phi} &= \\left(\n",
    "    \\begin{array}{ccc}\n",
    "\t    \\phi_0(x_1) & \\cdots & \\phi_{M-1}(x_1) \\\\\n",
    "\t    \\vdots & \\ddots & \\vdots \\\\     \n",
    "\t    \\phi_0(x_N) & \\cdots & \\phi_{M-1}(x_N) \\\\\n",
    "    \\end{array}\n",
    "\t    \\right) \\text{with } \\boldsymbol{\\Phi}\\in\\mathbb{R}^{N\\times M},\t    \n",
    "  \\\\\n",
    "  \\\\\n",
    "   \\mathbf{w} &= \\left(\n",
    "    \\begin{array}{ccc}\n",
    "\t    w_0 \\\\\n",
    "\t    \\vdots\\\\     \n",
    "\t    w_{M-1} \\\\\n",
    "    \\end{array}\n",
    "\t    \\right) \\text{with } \\mathbf{w}\\in\\mathbb{R}^{M}, \\qquad\n",
    "\t\\mathbf{t} = \\left(\n",
    "    \\begin{array}{c}\n",
    "\t    t_1\\\\\n",
    "\t    \\vdots\\\\     \n",
    "\t    t_N\\\\\n",
    "    \\end{array}\n",
    "\t    \\right) \\text{with } \\mathbf{t}\\in\\mathbb{R}^{N}\n",
    " \\\\\n",
    " \\\\\n",
    " \\\\\n",
    "\t  \\boldsymbol{\\Phi} \\cdot \\mathbf{w} - \\mathbf{t} &= \\left(\n",
    "\t    \\begin{array}{ccc}\n",
    "\t\t    w_0 \\phi_0(x_1) + & \\cdots & + w_{M-1} \\phi_{M-1}(x_1)-y_1\\\\\n",
    "\t\t     & \\vdots & \\\\     \n",
    "\t\t    w_0 \\phi_0(x_N) + & \\cdots & + w_{M-1} \\phi_{M-1}(x_N)-y_N\\\\\n",
    "\t    \\end{array}\t\t                          \n",
    "        \\right)\n",
    "        \\\\\n",
    "        \\\\\n",
    "\t &= \\left(\n",
    "\t    \\begin{array}{c}\n",
    "\t\t    \\left(\\sum\\limits_{j=0}^{M-1} w_j \\phi_j(x_1)\\right)-y_1\\\\\n",
    "\t\t    \\vdots\\\\     \n",
    "\t\t    \\left(\\sum\\limits_{j=0}^{M-1} w_j \\phi_j(x_N)\\right)-y_N\\\\\n",
    "\t    \\end{array}\n",
    "\t\t\\right)\n",
    "        \\\\\n",
    "        \\\\\n",
    "        \\\\\n",
    "\t||\\boldsymbol{\\Phi} \\cdot \\mathbf{w} - \\mathbf{t}||^2\n",
    "\t\t&= \\left(\\bigg(\\sum\\limits_{j=0}^{M-1} w_j \\phi_j(x_1)\\bigg)-y_1\\right)^2\n",
    "\t\t\t+\\cdots\n",
    "\t\t\t+\\left(\\bigg(\\sum\\limits_{j=0}^{M-1} w_j \\phi_j(x_N)\\bigg)-y_N\\right)^2\\\\\t\t\t\n",
    "\t\t &= \\sum\\limits_{n=1}^N \\left(\\bigg(\\sum\\limits_{j=0}^{M-1} w_j \\phi_j(x_n)\\bigg)-y_n\\right)^2\n",
    "\\end{align}\n",
    "\n",
    "The alternative notation for the least squares minimization problem, $\\min ||\\boldsymbol{\\Phi} \\cdot \\mathbf{w} - \\mathbf{t}||^2$, is as follows:\n",
    "\n",
    "**Derivation**\n",
    "\n",
    "\\begin{align}\n",
    "\t&\\frac{\\partial}{\\partial \\mathbf{w}} ||\\boldsymbol{\\Phi} \\cdot \\mathbf{w} - \\mathbf{t}||^2 = 2 \\cdot \\boldsymbol{\\Phi}^\\mathrm{T} \\cdot (\\boldsymbol{\\Phi} \\cdot \\mathbf{w} - \\mathbf{t}) \\qquad\\text{(chain rule)}&\n",
    "\\end{align}\n",
    "\n",
    "Set the derivative equal to 0 to find the optimal solution $\\mathbf{w}^{opt}$:\n",
    "\n",
    "\\begin{align}\n",
    "\t&2 \\boldsymbol{\\Phi}^\\mathrm{T} \\cdot (\\boldsymbol{\\Phi} \\cdot \\mathbf{w}^{opt} - \\mathbf{t}) = 0 &\\\\\n",
    "\t&2 \\boldsymbol{\\Phi}^\\mathrm{T} \\cdot \\boldsymbol{\\Phi} \\cdot \\mathbf{w}^{opt} - 2 \\boldsymbol{\\Phi}^\\mathrm{T} \\cdot \\mathbf{t} = 0&\\\\\n",
    "\t&\\boldsymbol{\\Phi}^\\mathrm{T} \\cdot \\boldsymbol{\\Phi} \\cdot \\mathbf{w}^{opt} = \\boldsymbol{\\Phi}^\\mathrm{T} \\cdot \\mathbf{t}&\n",
    "\\end{align}\n",
    "\n",
    "$\\boldsymbol{\\Phi}^\\mathrm{T} \\boldsymbol{\\Phi}$ is quadratic and must be invertible (must be assured, e.g., through the choice of $\\phi_j$), then multiplication on both sides from the left with $(\\boldsymbol{\\Phi}^\\mathrm{T} \\boldsymbol{\\Phi})^{-1}$:\n",
    "\n",
    "\\begin{align}\n",
    "\t&\\mathbf{w^{opt}} = (\\boldsymbol{\\Phi}^\\mathrm{T} \\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^\\mathrm{T} \\cdot \\mathbf{t} \\qquad \\text{with: } \\boldsymbol{\\Phi}^\\dagger=(\\boldsymbol{\\Phi}^\\mathrm{T} \\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^\\mathrm{T} \\text{ is the pseudo-inverse of }\\boldsymbol{\\Phi}.\n",
    "\\end{align}\n",
    "\n",
    "$\\mathbf{w}^{opt} = \\boldsymbol{\\Phi}^\\dagger \\cdot \\mathbf{t}$ is the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbitrary Basis Functions and Samples in $\\mathbb{R}$\n",
    "\n",
    "### Your tasks:\n",
    "1. learn the linear model with the method of least squares for polynomial, sine, and cosine basis functions\n",
    "2. display the resulting trained parameters\n",
    "3. plot the data points\n",
    "3. plot the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Parameters, change if you like.\n",
    "\n",
    "# Number of data samples for fitting\n",
    "N_target = 30\n",
    "\n",
    "# Number of inputs for prediction.\n",
    "N_predict = 100\n",
    "\n",
    "# Noise properties\n",
    "noise_mu = 0\n",
    "noise_sigma = 0.05\n",
    "\n",
    "\n",
    "# The target function for one-dimensional input and one-dimensional output (R -> R)\n",
    "def targetfunction(x0):\n",
    "    return np.sin(0.5 * np.pi * x0) + 1.5\n",
    "\n",
    "\n",
    "# The ranges of x0 and x1\n",
    "x0_range_lower = 0\n",
    "x0_range_upper = 3\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# YOUR CODE HERE\n",
    "# ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrary Basis Functions and Samples from $\\mathbb{R^2}$\n",
    "\n",
    "### Your tasks:\n",
    "1. implement the following basis functions:\n",
    "  1. $\\sin(0.6*\\pi*\\sqrt{ (x_0)^2 + (x_1)^2} ) $\n",
    "  2. $\\cos(0.6*\\pi*\\sqrt{ (x_0)^2 + (x_1)^2} ) $\n",
    "1. learn the linear model with the method of least squares\n",
    "2. display the resulting trained parameters \n",
    "3. plot the data points\n",
    "3. plot the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Parameters, change if you like.\n",
    "\n",
    "# Number of data samples for fitting\n",
    "N_target = 100\n",
    "\n",
    "# Number of inputs per input dimension for prediction.\n",
    "N_predict = 100\n",
    "\n",
    "# Noise properties\n",
    "noise_mu = 0\n",
    "noise_sigma = 0.05\n",
    "\n",
    "\n",
    "# The target function for two-dimensional input and one-dimensional output (R^2 -> R)\n",
    "def targetfunction(x0, x1):\n",
    "    return np.sin(0.5 * np.pi * np.sqrt(np.square(x0) + np.square(x1))) + 1.5\n",
    "\n",
    "\n",
    "# The ranges of x0 and x1\n",
    "x0_range_lower = 0\n",
    "x0_range_upper = 3\n",
    "x1_range_lower = 0\n",
    "x1_range_upper = 3\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# YOUR CODE HERE\n",
    "# ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Gaussian Distribution\n",
    "\n",
    "The **Gaussian** distribution (aka **normal** distribution) is a common continuous probability distribution. Gaussian distributions are important in statistics and often used in the natural and social sciences to represent real-valued random variables. A random variable with a Gaussian distribution is said to be normally distributed and is called a normal deviate.\n",
    "\n",
    "The normal distribution is useful because of the **central limit theorem**. In its most general form, under some mild conditions, e.g., finite variance, it states that averages of samples of observations of random variables independently drawn from independent distributions become normally distributed when the number of observations is sufficiently large.\n",
    "\n",
    "The probability density function of the normal distribution of a random variable $x\\ \\epsilon\\ \\mathbb{R}$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathcal{N}\\left(x\\,|\\,\\boldsymbol{\\mu},\\boldsymbol{\\sigma}^2\\right) = \\frac{1}{(2\\pi\\ \\boldsymbol{\\sigma}^2)^{{1}/{2}}} \\exp\\left(-\\frac{1}{2\\ \\boldsymbol{\\sigma}^2} \\left(x - \\boldsymbol{\\mu}\\right)^2 \\right)\n",
    "\\end{equation}\n",
    "\n",
    "with  \n",
    "${\\mu}$: **mean** or expectation of the distribution  \n",
    "${\\sigma}$: **standard deviation**  \n",
    "${\\sigma}^2$: **variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "The following plot shows the probability density functions (PDF) of normal distributions with means $\\mathbf{\\mu} = [-5,0,5]$ and standard deviations $\\mathbf{\\sigma} = [0.5,2,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "# create x values\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "# set plot size\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "# create plots\n",
    "for mu, sigma in [(-5, 0.5), (0, 2), (5, 1)]:\n",
    "    plt.plot(x,scipy.stats.norm.pdf(x, mu, sigma))\n",
    "\n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "The following two plots visualize the central limit theorem.\n",
    "\n",
    "The first plot shows the probability density distribution of a single dice rolled 50000 times.\n",
    "The probability to roll a certain number is close to equally distributed between all numbers.  \n",
    "\n",
    "The second plot shows the probability density distribution of the sum of four dice rolled 50000 times.\n",
    "While the results of the single dice rolls are equally distributed, the distribution of the sum of four independently rolled dice is shaped Gaussian.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed()\n",
    "\n",
    "# rolls of a single dice\n",
    "\n",
    "counter = np.zeros(6)\n",
    "for rolls in range(50000) :\n",
    "    roll = np.random.randint(1,7)\n",
    "    counter[roll-1] = counter[roll-1] +1\n",
    "\n",
    "counter = counter / 50000\n",
    "x = np.arange(1,7,1)\n",
    "plt.bar(x,counter)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed()\n",
    "\n",
    "# sum of rolls of four dice\n",
    "\n",
    "counter = np.zeros(24)\n",
    "for rolls in range(50000) :\n",
    "    roll1 = np.random.randint(1,7)\n",
    "    roll2 = np.random.randint(1,7)\n",
    "    roll3 = np.random.randint(1,7)\n",
    "    roll4 = np.random.randint(1,7)\n",
    "    counter[roll1+roll2+roll3+roll4-1] = counter[roll1+roll2+roll3+roll4-1] +1\n",
    "\n",
    "counter = counter / (4*50000)\n",
    "x = np.arange(1,25,1)\n",
    "plt.bar(x,counter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "\n",
    "Maximum likelihood estimation (MLE) is a method of estimating the parameters of a probabilistic model based on observations {$\\mathbf{x}_n$} and their corresponding target values {${t}_n$}.\n",
    "The observations must be independently and identically distributed (\\textbf{iid}).\n",
    "MLE attempts to find the parameter values $\\mathbf{w}_{ML}$ that maximize the likelihood function for the given observations.\n",
    "\n",
    "\n",
    "## Maximum Likelihood for Linear Regression\n",
    "\n",
    "We assume that the target variable $t$ is given by a **deterministic function** $y(\\mathbf{x},\\mathbf{w})$ with an **added Gaussian noise**, modelled with a zero mean Gaussian random variable $\\varepsilon$.\n",
    "A real world example of this approach would be a physical experiment with measurements affected by a normally distributed statistical error.\n",
    "\n",
    "\\begin{equation}\n",
    "  t = y(\\mathbf{x},\\mathbf{w}) + \\varepsilon\n",
    "\\end{equation}\n",
    "\n",
    "The precision (inverse variance) of $\\varepsilon$ called $\\beta$.  \n",
    "With y as mean and $\\beta$ as precision we can write the probability of ${t}$ given $\\mathbf{x},\\mathbf{w}$, and $\\beta$ as a Gaussian distribution,\n",
    "\n",
    "\\begin{equation}  \n",
    "  p(t|\\mathbf{x},\\mathbf{w},\\beta) = \\mathcal{N}(t|y(\\mathbf{x},\\mathbf{w}), \\beta^{-1})\n",
    "\\end{equation}\n",
    "\n",
    "where, again, $y$ is a linear combination of basis functions ${\\phi}_{0}(\\mathbf{x}),\\ldots,{\\phi}_{M-1}(\\mathbf{x})$.\n",
    "\n",
    "We now consider a data set of inputs $\\mathbf{X}= \\{\\mathbf{x}_1, \\ldots \\mathbf{x}_N\\}$ and corresponding target values $\\{{t}_n\\} = \\{t_1, \\ldots ,t_N\\}$, written as column vector $\\mathsf{t}$.\n",
    "With the assumption that the data points are drawn independently from the above distribution (**identically and independently distributed**), we can write the so-called likelihood $p(\\mathbf{t}|\\mathbf{X},\\mathbf{w},\\beta)$ as a product:\n",
    "\n",
    "\\begin{equation}\n",
    "  p(\\boldsymbol{\\mathsf{t}}|\\mathbf{X},\\mathbf{w},\\beta) = \\prod_{n = 1}^{N}\\mathcal{N}(t_n|\\mathbf{w}^\\mathrm{T}\\boldsymbol{\\phi}(\\mathbf{x}_n),\\beta^{-1})\n",
    "\\end{equation}\n",
    "\n",
    "We use the **log likelihood** to turn the **product into a sum**. This is allowed because the logarithm is monotonous and minima and maxima remain at the same position.\n",
    "In order to keep the notation uncluttered, we drop the explicit $\\mathbf{X}$ from expressions like $p(\\mathbf{t}|\\mathbf{X},\\mathbf{w},\\beta)$.\n",
    "\n",
    "\\begin{align}\n",
    "  \\ln p(\\boldsymbol{\\mathsf{t}}|\\mathbf{w},\\beta) & = & \\sum_{n = 1}^{N}\\ln\\mathcal{N}(t_n|\\mathbf{w}^\\mathrm{T}\\boldsymbol{\\phi}(\\mathbf{x}_n),\\beta^{-1}) \\\\\n",
    "  & = & \\frac{N}{2}\\ln\\beta - \\frac{N}{2}\\ln(2\\pi) - \\beta E_D(\\mathbf{w})\n",
    "\\end{align}\n",
    "\n",
    "with the sum-of-squares error function:\n",
    "\n",
    "\\begin{equation}\n",
    "  E_D(\\mathbf{w}) = \\frac{1}{2}\\sum_{n = 1}^{N}\\left\\{t_n - \\mathbf{w}^\\mathrm{T}\\boldsymbol{\\phi}(\\mathbf{x}_n)\\right\\}^2 .\n",
    "\\end{equation}\n",
    "\n",
    "From this equation we see that maximizing the likelihood with respect to $\\mathbf{w}$ is equivalent to minimizing (due to the factor -1) a least squares error. To maximize the likelihood, we have to compute the gradient of the log likelihood function:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\nabla\\ln p(\\boldsymbol{\\mathsf{t}}|\\mathbf{w},\\beta) = \\beta \\cdot \\sum_{n=1}^N\\left\\{t_n - \\mathbf{w}^\\mathrm{T}\\boldsymbol{\\phi}(\\mathbf{x}_n)\\right\\}\\boldsymbol{\\phi}(\\mathbf{x}_n)^\\mathrm{T} .\n",
    "\\end{equation}\n",
    "\n",
    "Setting the gradient to zero gives\n",
    "\n",
    "\\begin{equation}\n",
    "  0 = \\sum_{n = 1}^{N}t_n\\boldsymbol{\\phi}(\\mathbf{x}_n)^\\mathrm{T} - \\mathbf{w}^\\mathrm{T}\\left(\\sum_{n = 1}^{N}\\boldsymbol{\\phi}(\\mathbf{x}_n)\\boldsymbol{\\phi}(\\mathbf{x}_n)^\\mathrm{T}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "Remember that $\\boldsymbol{\\phi}(\\mathbf{x}_n)\\boldsymbol{\\phi}(\\mathbf{x}_n)^\\mathrm{T}$ is a matrix, so we need to sum up $N$ matrices.\n",
    "\n",
    "Solving for $\\mathbf{w}$ we obtain:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathbf{w}_{\\mathrm{ML}} = \\left(\\boldsymbol{\\Phi}^\\mathrm{T}\\boldsymbol{\\Phi}\\right)^{-1}\\boldsymbol{\\Phi}^\\mathrm{T}\\boldsymbol{\\mathsf{t}}\n",
    "\\end{equation}\n",
    "\n",
    "which are known as the *normal equations* for the least squares problem. $\\boldsymbol{\\Phi}$ is an $N \\times M$ matrix called the *design* matrix. Its elements are given by $\\Phi_{nj} = \\phi_{j}(\\mathbf{x}_n)$, so that\n",
    "\n",
    "\\begin{equation}\n",
    "  \\boldsymbol{\\Phi} = \\left( \\begin{array}{cccc}\n",
    "  \\phi_0(\\mathbf{x}_1) & \\phi_1(\\mathbf{x}_1) & \\cdots & \\phi_{M-1}(\\mathbf{x}_1) \\\\\n",
    "  \\phi_0(\\mathbf{x}_2) & \\phi_1(\\mathbf{x}_2) & \\cdots & \\phi_{M-1}(\\mathbf{x}_2) \\\\\n",
    "  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "  \\phi_0(\\mathbf{x}_N) & \\phi_1(\\mathbf{x}_N) & \\cdots & \\phi_{M-1}(\\mathbf{x}_N)\n",
    "  \\end{array} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "$N$ rows of observations (samples), $M$ columns of basis functions\n",
    "\n",
    "The matrix \n",
    "\n",
    "\\begin{equation}\n",
    "  \\boldsymbol{\\Phi}^\\dagger\\equiv\\left(\\boldsymbol{\\Phi}^\\mathrm{T}\\boldsymbol{\\Phi}\\right)^{-1}\\boldsymbol{\\Phi}^\\mathrm{T}\n",
    "\\end{equation}\n",
    "\n",
    "is known as (Moore-Penrose) pseudo inverse of the matrix $\\boldsymbol{\\Phi}$.\n",
    "\n",
    "<!--\n",
    "\\begin{equation}\n",
    "  \\boldsymbol{\\Phi}^{\\dagger} \\equiv ( \\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} )^{-1} \\boldsymbol{\\Phi}^T\n",
    "\\end{equation}\n",
    "-->\n",
    "\n",
    "With ${\\phi}_{0}(\\mathbf{x}) = 1$ and ${\\phi}_{1}(\\mathbf{x}) = x_1$ in the two-dimensional case,\n",
    "equation (8) is equivalent to equations (9) and (10) of the least-squares solution for the regression line.\n",
    "That is $\\mathbf{w}_{\\mathrm{ML}}=\\mathbf{w}^{\\mathrm{opt}}$.\n",
    "\n",
    "## Hints & Practical Advice: Regularization\n",
    "\n",
    "If the term $(\\boldsymbol{\\Phi}^\\mathrm{T} \\boldsymbol{\\Phi})^{-1}$ from equation (11) is not invertible, one can add a weighted unit matrix $\\mathbf{I}$ of appropriate size, increasing the positive weight $\\lambda$ until the term becomes invertible.\n",
    "\n",
    "\\begin{equation}\n",
    "  ( \\boldsymbol{\\Phi}^\\mathrm{T} \\boldsymbol{\\Phi} ) +\\lambda\\ \\mathbf{I}\\ , \\lambda > 0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Fixed Basis Functions\n",
    "\n",
    "Difficulties can arise from the assumption that the basis functions are fixed before the data set is observed.\n",
    "As a consequence the **number of basis functions needs to grow rapidly**, often exponentially, with the **dimensionality** ${D}$ of the input data. This is a manifestation of the \"**curse of dimensionality**\".\n",
    "\n",
    "Approaches to avoid this are localized basis functions in radial basis function networks, support vector machines, and relevance vector machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of Orthogonal Basis Functions\n",
    "\n",
    "Wanted: $\\boldsymbol{\\Phi}^\\dagger\\equiv\\left(\\boldsymbol{\\Phi}^\\mathrm{T}\\boldsymbol{\\Phi}\\right)^{-1}\\boldsymbol{\\Phi}^\\mathrm{T}$\n",
    "\n",
    "The matrix to invert:\n",
    "\\begin{align}\n",
    "\t&\\boldsymbol{\\Phi} &=& \\left(\n",
    "\t    \\begin{array}{ccc}\n",
    "\t\t     \\phi_0(x_1) & \\cdots &  \\phi_{M-1}(x_1) \\\\\n",
    "\t\t    \\vdots & \\ddots & \\vdots \\\\     \n",
    "\t\t     \\phi_0(x_N) & \\cdots &  \\phi_{M-1}(x_N) \\\\\n",
    "\t    \\end{array}\n",
    "\t\t    \\right)\n",
    "\\end{align}\n",
    "\n",
    "Most of the computational effort goes into inverting $\\boldsymbol{\\Phi}^\\mathrm{T} \\boldsymbol{\\Phi}$.\n",
    "\n",
    "\\begin{align}\n",
    "\t\\boldsymbol{\\Phi}^\\mathrm{T} &= \\left(\n",
    "\t\t\\begin{array}{ccc}\n",
    "\t\t     \\phi_0(x_1) & \\cdots &  \\phi_0(x_N) \\\\\n",
    "\t\t    \\vdots & \\ddots & \\vdots \\\\     \n",
    "\t\t     \\phi_{M-1}(x_1) & \\cdots &  \\phi_{M-1}(x_N) \\\\\n",
    "\t\t\\end{array}\n",
    "\t\t    \\right)&\\\\\n",
    "\t\t\t\\\\\n",
    "\t\\boldsymbol{\\Phi}^\\mathrm{T} \\boldsymbol{\\Phi} &= \\left(\n",
    "\t\t\\begin{array}{ccc}\n",
    "\t\t    \\boldsymbol{\\Phi}^\\mathrm{T}(1,:)\\boldsymbol{\\Phi}(:,1) & \\cdots & \\boldsymbol{\\Phi}^\\mathrm{T}(1,:)\\boldsymbol{\\Phi}(:,M) \\\\\n",
    "\t\t    \\vdots & \\ddots & \\vdots \\\\     \n",
    "\t\t    \\boldsymbol{\\Phi}^\\mathrm{T}(N,:)\\boldsymbol{\\Phi}(:,1) & \\cdots & \\boldsymbol{\\Phi}^\\mathrm{T}(N,:)\\boldsymbol{\\Phi}(:,M)\\\\\n",
    "\t\t\\end{array}\n",
    "\t\t    \\right)\\quad \\text{mit $\\boldsymbol{\\Phi}$(row,column)}&\\\\\n",
    "\t\t    \\\\\n",
    "\t &=\n",
    "\t\t\\begin{pmatrix}\n",
    "\t\t    \\sum\\limits_{n=1}^N  \\phi_0(x_n)^2 & \\sum\\limits_{n=1}^N  \\phi_0(x_n) \\phi_1(x_n)\n",
    "\t\t    \t&  \\cdots &  \\sum\\limits_{n=1}^N  \\phi_0(x_n) \\phi_{M-1}(x_n)\\\\\n",
    "\t\t    \\sum\\limits_{n=1}^N  \\phi_1(x_n) \\phi_0(x_n) & \\sum\\limits_{n=1}^N  \\phi_1(x_n)^2\n",
    "\t\t    \t&  \\cdots & \\sum\\limits_{n=1}^N  \\phi_1(x_n) \\phi_{M-1}(x_n) \\\\     \n",
    "\t\t     \\vdots &  \\ddots &  \\ddots \n",
    "\t\t    \t& \\vdots \\\\\n",
    "\t\t    \\sum\\limits_{n=1}^N  \\phi_{M-1}(x_n) \\phi_0(x_n) & \\cdots\n",
    "\t\t    \t& \\cdots& \\sum\\limits_{n=1}^N  \\phi_{M-1}(x_n)^2\\\\\n",
    "\t\t\\end{pmatrix}\n",
    "\t\t    &\n",
    "\\end{align}\n",
    "\n",
    "Using basis finctions $\\phi$ that are pairwise orthogonal, i.e., for $ \\phi_{k}$ and $ \\phi_{l}$ with $k \\neq l$, applies $\\sum\\limits_{n=1}^N  \\phi_{k}(x_n) \\phi_{l}(x_n) = 0$.\\\\\n",
    "For comparison see the case of two orthogonal vectors, i.e. they are perpendicular to each other.\n",
    "The dot product of these vectors is 0.\n",
    "\n",
    "With orthogonal basis functions,\n",
    "\n",
    "\\begin{align}\n",
    "\t&\\boldsymbol{\\Phi}^\\mathrm{T} \\boldsymbol{\\Phi} = \\left(\n",
    "\t\t\\begin{array}{ccc}\n",
    "\t\t    \\sum\\limits_{n=1}^N  \\phi_0(x_n)^2 & & 0\\\\\n",
    "\t\t    & \\ddots &  \\\\   \n",
    "\t\t    0& & \\sum\\limits_{n=1}^N  \\phi_{M-1}(x_n)^2\\\\\n",
    "\t\t\\end{array}\n",
    "\t\t    \\right)&\n",
    "\\end{align} is a diagonal matrix and, therefore, **easily invertible**.\n",
    "\n",
    "With $|| \\phi_j||^2 = \\sum\\limits_{n=1}^N  \\phi_j(x_n)^2$ für $j=0,...,M-1$ the inverted of $\\boldsymbol{\\Phi}^\\mathrm{T} \\boldsymbol{\\Phi}$ is\n",
    "\n",
    "\\begin{align}\n",
    "\t&(\\boldsymbol{\\Phi}^\\mathrm{T} \\boldsymbol{\\Phi})^{-1} = \\left(\n",
    "\t\t\\begin{array}{ccc}\n",
    "\t\t    \\frac{1}{|| \\phi_0||^2} &  & 0\\\\\n",
    "\t\t     & \\ddots &  \\\\     \n",
    "\t\t    0&& \\frac{1}{|| \\phi_{M-1}||^2}\\\\\n",
    "\t\t\\end{array}\n",
    "\t\t    \\right)&\n",
    "\\end{align}\n",
    "\n",
    "Then the optimal solution is:\n",
    "\n",
    "\\begin{align}\n",
    "\t&\\mathbf{w^{opt}}&= \\;& \\mathbf{\\boldsymbol{\\Phi}^\\dagger}\\mathbf{y}&  &\\\\\n",
    "    \\\\\n",
    "\t&&= & (\\boldsymbol{\\Phi}^\\mathrm{T} \\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^\\mathrm{T}\\mathbf{y}&\\\\\n",
    "    \\\\\n",
    "\t&&= & \\left(\n",
    "\t\t\\begin{array}{ccc}\n",
    "\t\t    \\frac{1}{|| \\phi_0||^2} &  &0 \\\\\n",
    "\t\t     & \\ddots &  \\\\     \n",
    "\t\t    0&& \\frac{1}{|| \\phi_{M-1}||^2}\\\\\n",
    "\t\t\\end{array}\n",
    "\t\t    \\right)\\cdot \\left(\n",
    "\t\t\\begin{array}{ccc}\n",
    "\t\t\t \\phi_0(x_1) & \\cdots &  \\phi_0(x_N) \\\\\n",
    "\t\t\t\\vdots & \\ddots & \\vdots \\\\     \n",
    "\t\t\t \\phi_{M-1}(x_1) & \\cdots &  \\phi_{M-1}(x_N) \\\\\n",
    "\t\t\\end{array}\n",
    "\t\t\t\\right)\\cdot \\left(\n",
    "\t\t\\begin{array}{c}\n",
    "\t\t\ty_1\\\\\n",
    "\t\t\t\\vdots\\\\     \n",
    "\t\t\ty_N\\\\\n",
    "\t\t\\end{array}\n",
    "\t\t\t\\right)&\\\\\n",
    "            \\\\\n",
    "\t&&= & \\left(\n",
    "\t\t\\begin{array}{ccc}\n",
    "\t\t    \\frac{1}{|| \\phi_0||^2} & & 0\\\\\n",
    "\t\t    & \\ddots &  \\\\     \n",
    "\t\t    0& & \\frac{1}{|| \\phi_{M-1}||^2}\\\\\n",
    "\t\t\\end{array}\n",
    "\t\t    \\right) \\cdot \\left(\n",
    "\t\t\\begin{array}{c}\n",
    "\t\t   \t\\sum\\limits_{n=1}^N  \\phi_0(x_n)y_n\\\\\n",
    "\t\t    \\vdots\\\\\n",
    "\t\t    \\sum\\limits_{n=1}^N  \\phi_{M-1}(x_n)y_n\n",
    "\t\t\\end{array}\\right)&\\\\\n",
    "        \\\\\n",
    "\t&&= & \\left(\n",
    "\t\t\\begin{array}{c}\n",
    "\t\t   \t\\sum\\limits_{n=1}^N \\frac{y_n}{|| \\phi_0||^2} \\phi_0(x_n)\\\\\n",
    "\t\t   \t\\vdots\\\\\n",
    "\t\t   \t\\sum\\limits_{n=1}^N \\frac{y_n}{|| \\phi_{M-1}||^2} \\phi_{M-1}(x_n)\n",
    "\t\t\\end{array}\n",
    "\t\t    \\right)&\n",
    "\\end{align}\n",
    "\n",
    "Remark: The coefficients of the solution can be written as linear combination of the sample data.  \n",
    "The computational cost: $\\mathcal O(N\\cdot M)$\n",
    "\n",
    "How large is the error that is made?  \n",
    "(r: residue)\n",
    "\n",
    "\\begin{align}\n",
    "\t&r^{opt} &= \\;& ||\\boldsymbol{\\Phi} \\mathbf{w^{opt}} - \\mathbf{y}||^2 &\\\\\n",
    "\t&&= \\;& ||\\boldsymbol{\\Phi}\\mathbf{\\boldsymbol{\\Phi}^\\dagger}\\mathbf{y}-\\mathbf{y}||^2 \\quad \\text{(see binomial equations)} &\\\\\n",
    "\t&&= \\;& \\mathbf{y}^\\mathrm{T}(\\boldsymbol{\\Phi}\\boldsymbol{\\Phi}^\\mathrm{T})^\\mathrm{T} \\boldsymbol{\\Phi}\\mathbf{\\boldsymbol{\\Phi}^\\dagger}\\mathbf{y} - 2\\mathbf{y}^\\mathrm{T}\\boldsymbol{\\Phi}\\mathbf{\\boldsymbol{\\Phi}^\\dagger}\\mathbf{y} + \\mathbf{y}^\\mathrm{T}\\mathbf{y} &\n",
    "\\end{align}\n",
    "\n",
    "General properties of the pseudo inverse $\\mathbf{A^+}$ of $\\mathbf{A}$ are:\n",
    "1. $\\ (\\mathbf{A}\\mathbf{A}^\\dagger)^\\mathrm{T} = \\mathbf{A}\\mathbf{A}^\\dagger$\n",
    "2. $\\ \\mathbf{A}\\mathbf{A}^\\dagger\\mathbf{A} = \\mathbf{A}$, also $\\mathbf{A}\\mathbf{A}^\\dagger\\mathbf{A}\\mathbf{A}^\\dagger = \\mathbf{A}\\mathbf{A^T}$\n",
    "\n",
    "\n",
    ", therefore:\n",
    "\n",
    "\\begin{align}\n",
    "\t&r^{opt} &=\\;& \\mathbf{y}^\\mathrm{T}\\boldsymbol{\\Phi}\\mathbf{\\boldsymbol{\\Phi}^\\dagger}\\mathbf{y} - 2\\mathbf{y}^\\mathrm{T}\\boldsymbol{\\Phi}\\mathbf{\\boldsymbol{\\Phi}^\\dagger}\\mathbf{y} + \\mathbf{y}^\\mathrm{T}\\mathbf{y}\n",
    "    &\\\\\n",
    "\t& &=\\;& \\mathbf{y}^\\mathrm{T}\\mathbf{y} - \\mathbf{y}^\\mathrm{T}\\boldsymbol{\\Phi}\\mathbf{\\boldsymbol{\\Phi}^\\dagger}\\mathbf{y}&\n",
    "\\end{align}\n",
    "\n",
    "Generally:\n",
    "\\begin{align}\n",
    "&(\\mathbf{A}\\mathbf{B})^\\mathrm{T} = \\mathbf{B}^\\mathrm{T}\\mathbf{A}^\\mathrm{T}\\\\\n",
    "&(\\mathbf{A}^\\mathrm{T})^\\mathrm{T} = \\mathbf{A}\\\\\n",
    "&(\\mathbf{A}\\mathbf{A}^\\mathrm{T})^\\mathrm{T} = \\mathbf{A}\\mathbf{A}^\\mathrm{T}\\\\\n",
    "&(\\mathbf{A}^\\mathrm{T}\\mathbf{A})^\\mathrm{T} = \\mathbf{A}^\\mathrm{T}\\mathbf{A}\n",
    "\\end{align}\n",
    "\n",
    "and with\n",
    "\n",
    "\\begin{align}\n",
    "\t&\\mathbf{\\boldsymbol{\\Phi}^\\dagger}\\mathbf{y} &=& (\\boldsymbol{\\Phi}^\\mathrm{T}\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^\\mathrm{T}\\mathbf{y}&\\\\\n",
    "\t&\\boldsymbol{\\Phi}^\\mathrm{T}\\mathbf{y} &=& (\\boldsymbol{\\Phi}^\\mathrm{T}\\boldsymbol{\\Phi})(\\boldsymbol{\\Phi}^\\mathrm{T}\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^\\mathrm{T}\\mathbf{y}&\\\\\n",
    "\t&&=& (\\boldsymbol{\\Phi}^\\mathrm{T}\\boldsymbol{\\Phi})\\mathbf{\\boldsymbol{\\Phi}^\\dagger}\\mathbf{y} \n",
    "    &\\\\\n",
    "\t&&=& (\\boldsymbol{\\Phi}^\\mathrm{T}\\boldsymbol{\\Phi})\\mathbf{w^{opt}}&\\\\\n",
    "\t&\\mathbf{y}^\\mathrm{T}\\boldsymbol{\\Phi}&=& (\\boldsymbol{\\Phi}^\\mathrm{T}\\mathbf{y})^\\mathrm{T}&\\\\\n",
    "\t&&=& ((\\boldsymbol{\\Phi}^\\mathrm{T}\\boldsymbol{\\Phi})\\mathbf{w^{opt}})^\\mathrm{T}&\\\\\n",
    "\t&&=& \\mathbf{w^{{opt}^T}}(\\boldsymbol{\\Phi}^\\mathrm{T}\\boldsymbol{\\Phi})^\\mathrm{T}&\\\\\n",
    "\t&&=& \\mathbf{w^{opt}}(\\boldsymbol{\\Phi}^\\mathrm{T}\\boldsymbol{\\Phi})&\n",
    "\\end{align}\n",
    "\n",
    ", the following applies:\n",
    "\n",
    "$r^{opt} = \\mathbf{y}^\\mathrm{T}\\mathbf{y} - \\mathbf{w^{{opt}^T}}\\boldsymbol{\\Phi}^\\mathrm{T}\\boldsymbol{\\Phi} \\,\\mathbf{w^{opt}}$\n",
    "\n",
    "To get the average squared error we divide by the number of samples N:\n",
    "\n",
    "\\begin{align}\n",
    "\t&\\sigma_{opt}^2 = \\frac{1}{N} \\cdot r^{opt}&\n",
    "\\end{align}\n",
    "\n",
    "For the oththogonal function defined as $w_k = \\sum\\limits_{n=1}^N \\frac{y_n}{|| \\phi_j||^2} \\phi_j(x_n)$ für $j=0,...,M-1$ (element of the solution vector $\\mathbf{w^{opt}}$) we can show that:\n",
    "\n",
    "\\begin{align}\n",
    "\t&\\sigma_{opt}^2 = \\frac{1}{N} \\bigg(\\sum\\limits_{n=1}^N y_n^2 - \\sum\\limits_{j=0}^{M-1} w_j^2|| \\phi_j||^2\\bigg)&\n",
    "\\end{align}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
