{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General remarks\n",
    "This notebook introduces the mathematical foundations for the $C$-means clustering algorithm.\n",
    "It is up to **you** to:\n",
    "1. extend the notebook with python code to implement the $C$-means algorithm for two-dimensional input data.\n",
    "2. log properties like the distortion, iterations until convergence, and number of samples per cluster.\n",
    "3. to plot the data, the cluster they belong to, and the cluster centers for two-dimensional input.\n",
    "4. implement and plot the elbow method.\n",
    "\n",
    "and as bonus:\n",
    "1. recreate the special cases shown in examples 1 & 2.\n",
    "2. plot the development of the cluster centers from iteration to iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $C$-means Clustering\n",
    "\n",
    "* Problem: Identification of Groups or clusters of data points in a multidimensional space.\n",
    "* Given a dataset $\\{\\mathbf{x}_1,\\ldots\\mathbf{x}_N\\}$ containing $N$ observations of a $D$-dimensional Euclidean random variable $\\mathbf{x}$.\n",
    "* The goal is to separate the data points into $C$ clusters, considering first that $C$ is given.\n",
    "* A cluster can be considered as a group of data points, in which the distance between points of the same cluster is small compared to the distance of a point of the cluster and one outside the cluster.\n",
    "\n",
    "\n",
    "### Goal\n",
    "* We introduce a set of $D$-dimensional vectors $\\boldsymbol{\\mu}_c$ with $c=1,\\ldots,C$, where $\\boldsymbol{\\mu}_c$ is the *prototype* which is associated with the cluster $c$.\n",
    "* Intuitively we can imagine that $\\boldsymbol{\\mu}_c$ is the center (the focal point) of a cluster.\n",
    "* The goal is to assign data points to clusters and find a set of vectors $\\{\\boldsymbol{\\mu}_c\\}$ so that the sum of the squared distances of each data point to the closest vector $\\boldsymbol{\\mu}_c$ is minimal.\n",
    "\n",
    "\n",
    "### Data Point Assignment\n",
    "* To describe the assignment of data points to clusters, a set of binary indicator variables $r_{nk} \\in \\{0,1\\}$ with $c=1,\\ldots,C$ will be introduced for each data point $\\mathbf{x}_n$, and which describes to which cluster $C$ the point belongs.\n",
    "* If $\\mathbf{x}_n$ belongs to cluster $c$, then $r_{nc}=1$ and $r_{nj}=0$ are for all $j \\neq c$ (1-out of-$C$-Coding scheme).\n",
    "* The target function (*distortion measure*)\n",
    "\t$$J = \\sum_{n=1}^N \\sum_{c=1}^C r_{nc}\\lVert\\mathbf{x}_n-\\boldsymbol{\\mu}_c\\rVert^2 \n",
    "\t= \\sum_{n=1}^N \\sum_{c=1}^C r_{nc} (\\mathbf{x}_n-\\boldsymbol{\\mu}_c)^{\\mathrm{T}}(\\mathbf{x}_n-\\boldsymbol{\\mu}_c)$$\t\n",
    "\tthen describes the sum of squared distance between each data point to *its* vector $\\boldsymbol{\\mu}_c$.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "* The goal is to find the value of $\\{r_{nc}\\}$ and $\\{\\boldsymbol{\\mu}_c\\}$, or to minimize $J$.\n",
    "* This is possible in an iterative process consisting of two alternating steps, each of which operates optimizations regarding $r_{nc}$ or $\\boldsymbol{\\mu}_c$:\n",
    "    The initial values of $\\boldsymbol{\\mu}_c$ are first chosen.\n",
    "    - E (expectation): In the first step $J$ is minimized relative to $r_{nc}$, where the value of $\\boldsymbol{\\mu}_c$ is fixed.  \n",
    "    - M (maximization): In the second step $J$ is minimized relative to $\\boldsymbol{\\mu}_c$ where $r_{nc}$ is fixed.\n",
    "    The two steps are repeated until the process converges.\n",
    "* Terms E and M will later become clear.\n",
    "\n",
    "\n",
    "E-step: determine the value of $r_{nc}$\n",
    "* $J$ is a linear function of $r_{nc}$ and the optimization can be easily done.\n",
    "* The terms for different $n$ are independent and for each $n$, the value $r_{nc}=1$ can be chosen for the $c$ which gives the minimum value of $\\lVert\\mathbf{x}_n-\\boldsymbol{\\mu}_c\\rVert^2$ .\n",
    "* The $n$th data point can then be assigned to the nearest cluster center:\n",
    "\t$$r_{nk} = \\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t1 & \\,\\, \\mathrm{if} \\,\\, c = \\arg \\min_j \\lVert\\mathbf{x}_n-\\boldsymbol{\\mu}_j\\rVert^2\\\\\n",
    "\t0 & \\,\\, \\mathrm{otherwise} \n",
    "\t\\end{array}\\right.$$\n",
    "\n",
    "\n",
    "M-step: determine $\\boldsymbol{\\mu}_k$\n",
    "* $J$ is a quadratic function of $\\boldsymbol{\\mu}_c$.\n",
    "* The derivate after $\\boldsymbol{\\mu}_c$ and equate to zero yields:\n",
    "\t$$\\begin{eqnarray*}\n",
    "\t\t\\sum_{n=1}^N r_{nc} (\\mathbf{x}_n-\\boldsymbol{\\mu}_c) & = & 0\\\\\n",
    "\t\t\\boldsymbol{\\mu}_c & = & \\frac{\\sum_n r_{nc}\\mathbf{x}_n}{\\sum_n r_{nc}}\n",
    "\t\\end{eqnarray*}$$\n",
    "* The denominator is equal to the number of points assigned to the cluster. The value of $\\boldsymbol{\\mu}_c$ is also equal to the mean of all points $\\mathbf{x}_n$ which belong to the cluster $c$. \n",
    "* This method is therefore also called *$C$-means algorithm* (some literature also calls it *$K$-means algorithm*).\n",
    "\n",
    "\n",
    "* E-step and M-step are repeated until no change occurs in the assignment or that another break criterion (eg: maximum number of iterations) has been reached.\n",
    "* As the value of the target function $J$ is reduced in each step, convergence is guaranteed.\\pause\n",
    "* However convergence is also possible in a local minimum rather than in a global minimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Initialization\n",
    "\n",
    "The following two images show the importance of the initial choices for the cluster centers $\\boldsymbol{\\mu}_c$.\n",
    "Both images show the same data set consisting of three easily distinguishable clusters.\n",
    "The final positions of the centers are marked by a black x, the dotted lines show their movement from iteration to iteration of the $C$-means algorithm.\n",
    "\n",
    "In the left image, we initialize $C$-means with three centers $\\boldsymbol{\\mu}_c$.\n",
    "Each center is already close to one distinct cluster.\n",
    "The result is a global minimum of $J$.\n",
    "\n",
    "In the right image, we again initialize $C$-means with three centers $\\boldsymbol{\\mu}_c$.\n",
    "Now two centers are initially placed close to the larger cluster on the left and the third center close to the other two smaller clusters.\n",
    "The result is a local minimum of $J$, where two centers divide the larger cluster in two, and the data points of the two smaller clusters are all assigned to the third center.\n",
    "\n",
    "![Global minimum (left) and local minimum (right).](img_cmeans/init.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Scaling\n",
    "\n",
    "The following three images show the importance of the scaling of the different data dimensions.\n",
    "\n",
    "Both images show the data set from the file \"ScaleTest.csv\".\n",
    "It has three clusters. One large cluster around position (4.5, 0) and two smaller clusters above it, one to the left and one to the right.\n",
    "\n",
    "The initial choices for the cluster centers $\\boldsymbol{\\mu}_c$ are identical in both images.\n",
    "The only difference is in the scaling of $y$.\n",
    "The left image shows the original data, in the right image the $y$ values are scaled by a factor of 3.\n",
    "\n",
    "In experiments data scaling happens, e.g., due to the choice of the data unit.\n",
    "Consider data where one dimension is the weight and another dimension the height of test persons.\n",
    "Choosing kilogramms vs gramms for the weight, or meters vs centimeters for the height can make a noticable difference for the cluster detection.\n",
    "\n",
    "As in example 1, the final positions of the centers are marked by a black x, the dotted lines show their movement from iteration to iteration of the $C$-means algorithm.\n",
    "\n",
    "Please remember that due to the scaled $y$ axis and the term $\\lVert\\mathbf{x}_n-\\boldsymbol{\\mu}_c\\rVert^2$, the $y$-differences between data points and cluster centers go into the distortion $J$ with factor 9.\n",
    "Therefore, even though the cluster detection in the right image is visibly better, its distortion value is greater.\n",
    "\n",
    "![Original data (left) and $y$ axis scaled by factor 3 (right).](img_cmeans/scale_test.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Number of Centers $C$\n",
    "\n",
    "With optimally chosen cluster centers the distortion $J$ decreases or remains the same with an increasing number of cluster centers $C$.\n",
    "\n",
    "Assuming that each cluster center actually has data points assigned to it, setting the number of cluster centers $C$ equal to the number of data points $N$ would mean $J=0$. Each cluster center would have only one data point assigned and converge to exactly that point. The set of cluster centers would match the set of data points.\n",
    "\n",
    "Depending on the type of data, such a high number of cluster centers $C$ and the corresponding low number of data points per cluster might not always be useful.\n",
    "In addition, $C$ also goes linearly into the computational effort of the $C$-means algorithm.\n",
    "\n",
    "A heuristic for determining a suitable value for $C$ is the **Elbow method**.\n",
    "Starting at $C=1$ we compute the distortion $J_c$ for increasing $C$ in steps of one.\n",
    "To take the influence of $C$ on the computational complexity into account, we divide $J_c$ by $C$.\n",
    "\n",
    "$$\\epsilon = \\frac{J_c}{C}$$\n",
    "\n",
    "We then plot $\\epsilon$ over $C$.\n",
    "If the resulting graph looks like a bent arm, we choose the value of $C$ that is at the \"elbow\" of the arm.\n",
    "Increasing $C$ beyond this value would give only small improvements compared to the increase in computational effort.\n",
    "\n",
    "The following image shows $C$-means clustering of the data points from \"SemiCircle.csv\".\n",
    "$C$ ranges from 1 to 15, the initial cluster centers are randomly placed within the area covered by the data points.\n",
    "The last subplot shows $\\epsilon$ over $C$.\n",
    "From $C=1 \\text{ to } 3$ $\\epsilon$ drops quickly, after $3$ it hardly changes anymore.\n",
    "Therefore, according to the elbow method, the optimal number of cluster centers is $3$.\n",
    "\n",
    "![Original data (left) and $y$ axis scaled by factor 3 (right).](img_cmeans/epsilon.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       att1      att2  label\n",
      "0 -0.542542  3.076261      0\n",
      "1 -0.048166  3.203736      0\n",
      "2  0.352438  2.780910      0\n",
      "3  0.865153  3.216087      0\n",
      "4  1.287322  3.172421      0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# You can generate your own data, or read it from a file, e.g.:\n",
    "data = pd.read_csv(\"data_cmeans/SemiCircle.csv\")\n",
    "print(data.head())\n",
    "x = np.array(data['att1'].values).reshape(-1, 1)\n",
    "y = np.array(data['att2'].values).reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# YOUR CODE HERE\n",
    "# ... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
